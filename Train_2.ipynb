{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Code\\GitHub\\Process_Analytics\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 67ms/step - accuracy: 0.2596 - loss: 2.0803 - val_accuracy: 0.2671 - val_loss: 2.0357\n",
      "Epoch 2/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 74ms/step - accuracy: 0.2647 - loss: 2.0303 - val_accuracy: 0.2669 - val_loss: 2.0202\n",
      "Epoch 3/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 66ms/step - accuracy: 0.2658 - loss: 2.0168 - val_accuracy: 0.2669 - val_loss: 2.0149\n",
      "Epoch 4/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 67ms/step - accuracy: 0.2651 - loss: 2.0069 - val_accuracy: 0.2668 - val_loss: 2.0093\n",
      "Epoch 5/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2693 - loss: 2.0065"
     ]
    }
   ],
   "source": [
    "# ✅ Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking, Input, Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# ✅ Load Data\n",
    "file_path_activity = \"Sepsis_Merged_Selected_Features_Activity.csv\"\n",
    "file_path_biomarkers = \"Sepsis_Biomarkers_Next_Activity.csv\"\n",
    "df_activity = pd.read_csv(file_path_activity)\n",
    "df_biomarkers = pd.read_csv(file_path_biomarkers)\n",
    "\n",
    "# ✅ Create 'Activity_Sequence' Column by Merging Activity Columns\n",
    "activity_columns = [col for col in df_activity.columns if \"Activity\" in col]\n",
    "df_activity[\"Activity_Sequence\"] = df_activity[activity_columns].apply(lambda row: \" -> \".join(row.dropna().astype(str)), axis=1)\n",
    "\n",
    "# ✅ Merge Biomarkers with Activity Data on 'Case ID' to Ensure Inclusion\n",
    "df_merged = df_activity.merge(df_biomarkers, on=\"Case ID\", how=\"left\")\n",
    "\n",
    "# ✅ Create an Expanded Dataset Where Each Row Represents a Progressive Sequence\n",
    "expanded_data = []\n",
    "\n",
    "for _, row in df_merged.iterrows():\n",
    "    full_sequence = row[\"Activity_Sequence\"].split(\" -> \")\n",
    "    \n",
    "    for i in range(1, len(full_sequence)):  # Create progressive sequences\n",
    "        input_seq = \" -> \".join(full_sequence[:i])  # Keep increasing sequence length\n",
    "        next_activity = full_sequence[i]  # The next step in the sequence\n",
    "        \n",
    "        expanded_data.append({\n",
    "            \"Case ID\": row[\"Case ID\"],\n",
    "            \"Input_Sequence\": input_seq,\n",
    "            \"Next_Activity\": next_activity,\n",
    "            \"Biomarker\": row[\"Biomarker\"],\n",
    "            \"Biomarker_Value\": row[\"Value\"],\n",
    "            \"Biomarker_Range\": row[\"Range\"]\n",
    "        })\n",
    "\n",
    "# ✅ Convert to DataFrame\n",
    "df_expanded = pd.DataFrame(expanded_data)\n",
    "\n",
    "# ✅ Encode Next Activity as the Target Variable\n",
    "label_encoder = LabelEncoder()\n",
    "df_expanded[\"Next_Activity_Encoded\"] = label_encoder.fit_transform(df_expanded[\"Next_Activity\"])\n",
    "\n",
    "# ✅ Tokenize Sequences\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(df_expanded[\"Input_Sequence\"])\n",
    "sequences = tokenizer.texts_to_sequences(df_expanded[\"Input_Sequence\"])\n",
    "max_sequence_length = max(map(len, sequences))\n",
    "X_seq = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# ✅ Select Relevant Features (Including Biomarkers)\n",
    "selected_features = [\n",
    "    \"SIRSCriteria2OrMore\", \"Infusion\", \"SIRSCritTemperature\", \"DiagnosticLacticAcid\",\n",
    "    \"SIRSCritHeartRate\", \"DiagnosticXthorax\", \"SIRSCritTachypnea\",\n",
    "    \"DiagnosticUrinarySediment\", \"Age\", \"InfectionSuspected\"\n",
    "]\n",
    "\n",
    "# ✅ Merge Features from df_merged Based on 'Case ID'\n",
    "df_expanded = df_expanded.merge(df_merged[[\"Case ID\"] + selected_features], on=\"Case ID\", how=\"left\")\n",
    "\n",
    "# ✅ Ensure the Feature Dataset Matches the Sequence Dataset\n",
    "X_features = df_expanded[selected_features]\n",
    "\n",
    "# ✅ Normalize Features\n",
    "scaler = StandardScaler()\n",
    "X_features = pd.DataFrame(scaler.fit_transform(X_features), columns=selected_features)\n",
    "\n",
    "# ✅ Convert Next Activity to One-Hot Encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_seq = tf.keras.utils.to_categorical(df_expanded[\"Next_Activity_Encoded\"], num_classes=num_classes)\n",
    "\n",
    "# ✅ Ensure All Datasets Have the Same Length Before Training\n",
    "min_length = min(len(X_seq), len(X_features), len(y_seq))\n",
    "X_seq = X_seq[:min_length]\n",
    "X_features = X_features[:min_length]\n",
    "y_seq = y_seq[:min_length]\n",
    "\n",
    "# ✅ Split Data into Training and Testing Sets\n",
    "X_train_seq, X_test_seq, X_train_features, X_test_features, y_train, y_test = train_test_split(\n",
    "    X_seq, X_features, y_seq, test_size=0.2, random_state=42, stratify=y_seq\n",
    ")\n",
    "\n",
    "# ✅ Define the LSTM-Based Model\n",
    "sequence_input = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_sequence_length)(sequence_input)\n",
    "masking_layer = Masking(mask_value=0.0)(embedding_layer)\n",
    "lstm_layer = LSTM(128, return_sequences=False, dropout=0.2)(masking_layer)\n",
    "\n",
    "feature_input = Input(shape=(len(selected_features),))\n",
    "feature_dense = Dense(32, activation='relu')(feature_input)\n",
    "\n",
    "merged = Concatenate()([lstm_layer, feature_dense])\n",
    "dense_layer = Dense(64, activation='relu')(merged)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "# ✅ Compile the Model\n",
    "model = Model(inputs=[sequence_input, feature_input], outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ✅ Train the Model\n",
    "history = model.fit([X_train_seq, X_train_features], y_train, validation_data=([X_test_seq, X_test_features], y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# ✅ Evaluate the Model\n",
    "test_loss, test_accuracy = model.evaluate([X_test_seq, X_test_features], y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Example Prediction Function\n",
    "def predict_next_activity(activity_sequence, feature_values):\n",
    "    sequence = tokenizer.texts_to_sequences([activity_sequence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    feature_array = np.array(feature_values).reshape(1, -1)\n",
    "    feature_array = scaler.transform(pd.DataFrame(feature_array, columns=selected_features))\n",
    "\n",
    "    model_prediction = model.predict([padded_sequence, feature_array])\n",
    "    predicted_class = np.argmax(model_prediction, axis=1)\n",
    "    return label_encoder.inverse_transform(predicted_class)[0]\n",
    "\n",
    "# ✅ Example Usage\n",
    "example_sequence = \"ER Registration -> ER Triage -> Leucocytes\"\n",
    "example_features = [1, 1, 1, 1, 1, 0, 0, 1, 50, 1]\n",
    "\n",
    "predicted_next_activity = predict_next_activity(example_sequence, example_features)\n",
    "print(f\"Predicted Next Activity: {predicted_next_activity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
