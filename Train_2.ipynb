{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1  align='center'>Sepsis Next Activity and Remaining Time Prediction Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1  align='center'>Model Training & Prediction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking, Input, Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import save_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Load Data\n",
    "file_path_activity = \"Sepsis_Merged_Selected_Features_Activity.csv\"\n",
    "file_path_biomarkers = \"Sepsis_Biomarkers_Next_Activity.csv\"\n",
    "df_activity = pd.read_csv(file_path_activity)\n",
    "df_biomarkers = pd.read_csv(file_path_biomarkers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Create 'Activity_Sequence' Column by Merging Activity Columns\n",
    "activity_columns = [col for col in df_activity.columns if \"Activity\" in col]\n",
    "df_activity[\"Activity_Sequence\"] = df_activity[activity_columns].apply(lambda row: \" -> \".join(row.dropna().astype(str)), axis=1)\n",
    "\n",
    "# ✅ Merge Biomarkers with Activity Data on 'Case ID' to Ensure Inclusion\n",
    "df_merged = df_activity.merge(df_biomarkers, on=\"Case ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Create an Expanded Dataset Where Each Row Represents a Progressive Sequence\n",
    "expanded_data = []\n",
    "\n",
    "for _, row in df_merged.iterrows():\n",
    "    full_sequence = row[\"Activity_Sequence\"].split(\" -> \")\n",
    "    \n",
    "    for i in range(1, len(full_sequence)):  # Create progressive sequences\n",
    "        input_seq = \" -> \".join(full_sequence[:i])  # Keep increasing sequence length\n",
    "        next_activity = full_sequence[i]  # The next step in the sequence\n",
    "        \n",
    "        expanded_data.append({\n",
    "            \"Case ID\": row[\"Case ID\"],\n",
    "            \"Input_Sequence\": input_seq,\n",
    "            \"Next_Activity\": next_activity,\n",
    "            \"Biomarker\": row[\"Biomarker\"],\n",
    "            \"Biomarker_Value\": row[\"Value\"],\n",
    "            \"Biomarker_Range\": row[\"Range\"]\n",
    "        })\n",
    "\n",
    "# ✅ Convert to DataFrame\n",
    "df_expanded = pd.DataFrame(expanded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Encode Next Activity as the Target Variable\n",
    "label_encoder = LabelEncoder()\n",
    "df_expanded[\"Next_Activity_Encoded\"] = label_encoder.fit_transform(df_expanded[\"Next_Activity\"])\n",
    "\n",
    "# ✅ Tokenize Sequences\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(df_expanded[\"Input_Sequence\"])\n",
    "sequences = tokenizer.texts_to_sequences(df_expanded[\"Input_Sequence\"])\n",
    "max_sequence_length = max(map(len, sequences))\n",
    "X_seq = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# ✅ Select Relevant Features (Including Biomarkers)\n",
    "selected_features = [\n",
    "    \"SIRSCriteria2OrMore\", \"Infusion\", \"SIRSCritTemperature\", \"DiagnosticLacticAcid\",\n",
    "    \"SIRSCritHeartRate\", \"DiagnosticXthorax\", \"SIRSCritTachypnea\",\n",
    "    \"DiagnosticUrinarySediment\", \"Age\", \"InfectionSuspected\"\n",
    "]\n",
    "\n",
    "# ✅ Merge Features from df_merged Based on 'Case ID'\n",
    "df_expanded = df_expanded.merge(df_merged[[\"Case ID\"] + selected_features], on=\"Case ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Ensure the Feature Dataset Matches the Sequence Dataset\n",
    "X_features = df_expanded[selected_features]\n",
    "\n",
    "# ✅ Normalize Features\n",
    "scaler = StandardScaler()\n",
    "X_features = pd.DataFrame(scaler.fit_transform(X_features), columns=selected_features)\n",
    "\n",
    "# ✅ Convert Next Activity to One-Hot Encoding\n",
    "num_classes = df_expanded[\"Next_Activity_Encoded\"].nunique()\n",
    "\n",
    "y_seq = tf.keras.utils.to_categorical(df_expanded[\"Next_Activity_Encoded\"], num_classes=num_classes)\n",
    "\n",
    "# ✅ Ensure All Datasets Have the Same Length Before Training\n",
    "min_length = min(len(X_seq), len(X_features), len(y_seq))\n",
    "X_seq = X_seq[:min_length]\n",
    "X_features = X_features[:min_length]\n",
    "y_seq = y_seq[:min_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Split Data into Training and Testing Sets\n",
    "X_train_seq, X_test_seq, X_train_features, X_test_features, y_train, y_test = train_test_split(\n",
    "    X_seq, X_features, y_seq, test_size=0.2, random_state=42, stratify=y_seq\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Code\\GitHub\\Process_Analytics\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Define the LSTM-Based Model\n",
    "sequence_input = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_sequence_length)(sequence_input)\n",
    "masking_layer = Masking(mask_value=0.0)(embedding_layer)\n",
    "lstm_layer = LSTM(128, return_sequences=False, dropout=0.2)(masking_layer)\n",
    "\n",
    "feature_input = Input(shape=(len(selected_features),))\n",
    "feature_dense = Dense(32, activation='relu')(feature_input)\n",
    "\n",
    "merged = Concatenate()([lstm_layer, feature_dense])\n",
    "dense_layer = Dense(64, activation='relu')(merged)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 62ms/step - accuracy: 0.2598 - loss: 2.0765 - val_accuracy: 0.2658 - val_loss: 2.0340\n",
      "Epoch 2/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 62ms/step - accuracy: 0.2652 - loss: 2.0311 - val_accuracy: 0.2583 - val_loss: 2.0276\n",
      "Epoch 3/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 61ms/step - accuracy: 0.2636 - loss: 2.0211 - val_accuracy: 0.2658 - val_loss: 2.0163\n",
      "Epoch 4/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 61ms/step - accuracy: 0.2676 - loss: 2.0147 - val_accuracy: 0.2702 - val_loss: 2.0118\n",
      "Epoch 5/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 61ms/step - accuracy: 0.2699 - loss: 2.0052 - val_accuracy: 0.2703 - val_loss: 2.0059\n",
      "Epoch 6/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 147ms/step - accuracy: 0.2724 - loss: 2.0005 - val_accuracy: 0.2695 - val_loss: 2.0030\n",
      "Epoch 7/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 61ms/step - accuracy: 0.2729 - loss: 2.0012 - val_accuracy: 0.2714 - val_loss: 2.0015\n",
      "Epoch 8/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 61ms/step - accuracy: 0.2734 - loss: 1.9976 - val_accuracy: 0.2716 - val_loss: 2.0016\n",
      "Epoch 9/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 61ms/step - accuracy: 0.2750 - loss: 1.9935 - val_accuracy: 0.2740 - val_loss: 2.0024\n",
      "Epoch 10/10\n",
      "\u001b[1m2957/2957\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 61ms/step - accuracy: 0.2728 - loss: 1.9960 - val_accuracy: 0.2742 - val_loss: 2.0031\n",
      "\u001b[1m740/740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.2746 - loss: 2.0012\n",
      "Test Accuracy: 0.27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Compile the Model\n",
    "model = Model(inputs=[sequence_input, feature_input], outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ✅ Train the Model\n",
    "history = model.fit([X_train_seq, X_train_features], y_train, validation_data=([X_test_seq, X_test_features], y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# ✅ Evaluate the Model\n",
    "test_loss, test_accuracy = model.evaluate([X_test_seq, X_test_features], y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_activity(activity_sequence, feature_values, biomarker_values):\n",
    "    \n",
    "\n",
    "    # Tokenize and pad the sequence\n",
    "    sequence = tokenizer.texts_to_sequences([activity_sequence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    # Convert feature values to DataFrame and scale\n",
    "    feature_array = np.array(feature_values).reshape(1, -1)\n",
    "    feature_array = scaler.transform(pd.DataFrame(feature_array, columns=selected_features))\n",
    "\n",
    "    # Ensure biomarker features match the training setup\n",
    "    biomarker_priority = [\"Leucocytes\", \"CRP\", \"LacticAcid\"]  # Biomarkers used in training\n",
    "    biomarker_feature_vector = np.zeros(3)  # Fixed size for 3 biomarker features\n",
    "\n",
    "    for i, biomarker in enumerate(biomarker_priority):\n",
    "        if biomarker in biomarker_values:\n",
    "            biomarker_range = biomarker_values[biomarker]\n",
    "            biomarker_feature_vector[i] = label_encoder.transform([biomarker_range])[0] if biomarker_range in label_encoder.classes_ else 0\n",
    "\n",
    "    # Check if model expects only clinical features (i.e., no biomarkers were included in training)\n",
    "    expected_feature_size = model.input_shape[1][1]  # Get expected feature count from model\n",
    "\n",
    "    if expected_feature_size == len(selected_features):  # Model was trained only on clinical features\n",
    "        full_feature_array = feature_array  # Ignore biomarker features\n",
    "    else:  # Model includes biomarker features\n",
    "        full_feature_array = np.concatenate((feature_array, biomarker_feature_vector.reshape(1, -1)), axis=1)\n",
    "\n",
    "    # Ensure feature array matches training dimensions\n",
    "    if full_feature_array.shape[1] != expected_feature_size:\n",
    "        raise ValueError(f\"Feature size mismatch! Expected {expected_feature_size}, but got {full_feature_array.shape[1]}.\")\n",
    "\n",
    "    # Predict using the model\n",
    "    model_prediction = model.predict([padded_sequence, full_feature_array])\n",
    "    predicted_class = np.argmax(model_prediction, axis=1)\n",
    "\n",
    "    return label_encoder.inverse_transform(predicted_class)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 18 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017A9D0C2E80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step\n",
      "Predicted Next Activity: Leucocytes\n"
     ]
    }
   ],
   "source": [
    "# Example activity sequence\n",
    "example_sequence = \"ER Registration -> ER Triage -> Leucocytes -> CRP -> LacticAcid\"\n",
    "\n",
    "# Example clinical feature values \n",
    "example_features_new = [1, 1, 1, 1, 1, 0, 0, 1, 50, 1]  \n",
    "\n",
    "# Example biomarker values (all expected biomarkers)\n",
    "biomarker_values = {\n",
    "    \"Leucocytes\": \"Elevated\",\n",
    "    \"CRP\": \"Severe\",\n",
    "    \"LacticAcid\": \"High\"\n",
    "}\n",
    "\n",
    "# Predict the next activity\n",
    "predicted_next_activity = predict_next_activity(\n",
    "    example_sequence, \n",
    "    example_features_new, \n",
    "    biomarker_values\n",
    ")\n",
    "\n",
    "print(f\"Predicted Next Activity: {predicted_next_activity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"activity_sequence\": \"ER Registration -> ER Triage -> Leucocytes -> CRP -> LacticAcid\",\n",
    "        \"feature_values\": [1, 1, 1, 1, 1, 0, 0, 1, 50, 1],\n",
    "        \"biomarker_values\": {\"Leucocytes\": \"High\", \"CRP\": \"Severe\", \"LacticAcid\": \"Normal\"}\n",
    "    },\n",
    "    {\n",
    "        \"activity_sequence\": \"ER Registration -> ER Triage -> Leucocytes -> CRP -> LacticAcid -> Leucocytes\",\n",
    "        \"feature_values\": [1, 1, 1, 1, 1, 0, 0, 1, 60, 1],\n",
    "        \"biomarker_values\": {\"Leucocytes\": \"High\", \"CRP\": \"Severe\", \"LacticAcid\": \"Normal\", \"Leucocytes\": \"Elevated\"}\n",
    "    },\n",
    "    {\n",
    "        \"activity_sequence\": \"ER Registration -> ER Triage -> CRP -> LacticAcid -> CRP\",\n",
    "        \"feature_values\": [0, 1, 1, 1, 1, 0, 0, 1, 40, 0],\n",
    "        \"biomarker_values\": {\"CRP\": \"Moderate\", \"LacticAcid\": \"Critical\", \"CRP\": \"Severe\"}\n",
    "    },\n",
    "    {\n",
    "        \"activity_sequence\": \"ER Registration -> ER Triage -> LacticAcid -> CRP -> LacticAcid\",\n",
    "        \"feature_values\": [1, 0, 1, 1, 1, 0, 0, 0, 55, 1],\n",
    "        \"biomarker_values\": {\"LacticAcid\": \"High\", \"CRP\": \"Severe\", \"LacticAcid\": \"Critical\"}\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Activity Sequence: ER Registration -> ER Triage -> Leucocytes -> CRP -> LacticAcid\n",
      "Feature Values: [1, 1, 1, 1, 1, 0, 0, 1, 50, 1]\n",
      "Biomarker Values: {'Leucocytes': 'High', 'CRP': 'Severe', 'LacticAcid': 'Normal'}\n",
      "Predicted Next Activity: Leucocytes\n",
      "\n",
      "Activity Sequence: ER Registration -> ER Triage -> Leucocytes -> CRP -> LacticAcid -> Leucocytes\n",
      "Feature Values: [1, 1, 1, 1, 1, 0, 0, 1, 60, 1]\n",
      "Biomarker Values: {'Leucocytes': 'Elevated', 'CRP': 'Severe', 'LacticAcid': 'Normal'}\n",
      "Predicted Next Activity: Leucocytes\n",
      "\n",
      "Activity Sequence: ER Registration -> ER Triage -> CRP -> LacticAcid -> CRP\n",
      "Feature Values: [0, 1, 1, 1, 1, 0, 0, 1, 40, 0]\n",
      "Biomarker Values: {'CRP': 'Severe', 'LacticAcid': 'Critical'}\n",
      "Predicted Next Activity: Leucocytes\n",
      "\n",
      "Activity Sequence: ER Registration -> ER Triage -> LacticAcid -> CRP -> LacticAcid\n",
      "Feature Values: [1, 0, 1, 1, 1, 0, 0, 0, 55, 1]\n",
      "Biomarker Values: {'LacticAcid': 'Critical', 'CRP': 'Severe'}\n",
      "Predicted Next Activity: Leucocytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run predictions for each test case using predict_next_activity\n",
    "predictions = []\n",
    "\n",
    "for test_case in test_cases:\n",
    "    activity_sequence = test_case[\"activity_sequence\"]\n",
    "    feature_values = test_case[\"feature_values\"]\n",
    "    biomarker_values = test_case[\"biomarker_values\"]\n",
    "    predicted_next_activity = predict_next_activity(activity_sequence, feature_values, biomarker_values)\n",
    "\n",
    "    predictions.append({\n",
    "        \"activity_sequence\": activity_sequence,\n",
    "        \"feature_values\": feature_values,\n",
    "        \"biomarker_values\": biomarker_values,\n",
    "        \"predicted_next_activity\": predicted_next_activity\n",
    "    })\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# Display the predictions\n",
    "\n",
    "for prediction in predictions:\n",
    "    print(f\"Activity Sequence: {prediction['activity_sequence']}\")\n",
    "    print(f\"Feature Values: {prediction['feature_values']}\")\n",
    "    print(f\"Biomarker Values: {prediction['biomarker_values']}\")\n",
    "    print(f\"Predicted Next Activity: {prediction['predicted_next_activity']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for Remaining Time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hskma\\AppData\\Local\\Temp\\ipykernel_15016\\2829901024.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_time[\"Complete Timestamp\"] = pd.to_datetime(df_time[\"Complete Timestamp\"], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "file_path_time = \"Sepsis_Cases_Log.csv\"\n",
    "df_time = pd.read_csv(file_path_time)\n",
    "\n",
    "# Convert timestamps to datetime format\n",
    "df_time[\"Complete Timestamp\"] = pd.to_datetime(df_time[\"Complete Timestamp\"], errors='coerce')\n",
    "df_time = df_time.dropna(subset=[\"Complete Timestamp\"])\n",
    "df_time = df_time.sort_values(by=[\"Case ID\", \"Complete Timestamp\"])\n",
    "\n",
    "# Compute duration between activities within each case\n",
    "df_time[\"Next Timestamp\"] = df_time.groupby(\"Case ID\")[\"Complete Timestamp\"].shift(-1)\n",
    "df_time[\"Activity Duration\"] = (df_time[\"Next Timestamp\"] - df_time[\"Complete Timestamp\"]).dt.total_seconds()\n",
    "df_time[\"Case Start Time\"] = df_time.groupby(\"Case ID\")[\"Complete Timestamp\"].transform(\"first\")\n",
    "df_time[\"Total Case Duration\"] = (df_time[\"Next Timestamp\"] - df_time[\"Case Start Time\"]).dt.total_seconds()\n",
    "df_time = df_time[[\"Case ID\", \"Activity\", \"Activity Duration\", \"Total Case Duration\"]].dropna()\n",
    "\n",
    "# Compute average remaining time per activity\n",
    "df_time_avg = df_time.groupby(\"Activity\")[\"Total Case Duration\"].mean().reset_index()\n",
    "df_time_avg.rename(columns={\"Total Case Duration\": \"Avg Remaining Time\"}, inplace=True)\n",
    "df_time = df_time.merge(df_time_avg, on=\"Activity\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForest Model for Remaining Time Prediction\n",
    "X_time = df_time[[\"Activity Duration\"]]\n",
    "y_time = df_time[\"Avg Remaining Time\"]\n",
    "X_train_time, X_test_time, y_train_time, y_test_time = train_test_split(X_time, y_time, test_size=0.2, random_state=42)\n",
    "time_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "time_model.fit(X_train_time, y_train_time)\n",
    "\n",
    "# Tokenizer for sequence processing\n",
    "tokenizer = Tokenizer()\n",
    "all_activities = df_time[\"Activity\"].unique().tolist()\n",
    "tokenizer.fit_on_texts(all_activities)\n",
    "max_sequence_length = max([len(tokenizer.texts_to_sequences([a])[0]) for a in all_activities])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Predict Next Activity and Remaining Time using NEW Selected Features\n",
    "def predict_next_activity_and_time_new(activity_sequence, feature_values, biomarker_values):\n",
    "    # Convert activity sequence to tokenized format\n",
    "    sequence = tokenizer.texts_to_sequences([activity_sequence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    # Predict the next activity using the updated features\n",
    "    predicted_next_activity = predict_next_activity(activity_sequence, feature_values, biomarker_values)\n",
    "\n",
    "    # Estimate Activity Duration from Historical Data\n",
    "    if predicted_next_activity in df_time_avg[\"Activity\"].values:\n",
    "        predicted_activity_duration = df_time_avg[df_time_avg[\"Activity\"] == predicted_next_activity][\"Avg Remaining Time\"].values[0]\n",
    "    else:\n",
    "        predicted_activity_duration = 600  # Default to 10 minutes if unknown\n",
    "\n",
    "    # Predict Remaining Time\n",
    "    predicted_remaining_time = time_model.predict([[predicted_activity_duration]])[0]\n",
    "    \n",
    "    return predicted_next_activity, round(predicted_remaining_time, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 19 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017A9D0C2E80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step\n",
      "Predicted Next Activity: Leucocytes, Predicted Remaining Time: 32473.04 seconds (~9.02 hours)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Code\\GitHub\\Process_Analytics\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage with New Features\n",
    "example_sequence = \"ER Registration -> ER Triage -> Leucocytes -> CRP -> LacticAcid\"\n",
    "example_features_new = [1, 1, 1, 1, 1, 0, 0, 1, 50, 1]  # Using only new selected features\n",
    "biomarker_values = {\"Leucocytes\": \"Elevated\", \"CRP\": \"Severe\", \"LacticAcid\": \"High\"}\n",
    "\n",
    "predicted_next_activity, predicted_remaining_time = predict_next_activity_and_time_new(\n",
    "    example_sequence, example_features_new, biomarker_values\n",
    ")\n",
    "\n",
    "print(f\"Predicted Next Activity: {predicted_next_activity}, Predicted Remaining Time: {predicted_remaining_time} seconds (~{predicted_remaining_time/3600:.2f} hours)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Code\\GitHub\\Process_Analytics\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Code\\GitHub\\Process_Analytics\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Code\\GitHub\\Process_Analytics\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Activity Sequence: ER Registration -> ER Triage -> Leucocytes -> CRP -> LacticAcid\n",
      "Feature Values: [1, 1, 1, 1, 1, 0, 0, 1, 50, 1]\n",
      "Biomarker Values: {'Leucocytes': 'High', 'CRP': 'Severe', 'LacticAcid': 'Normal'}\n",
      "Predicted Next Activity: ('Leucocytes', np.float64(32473.04))\n",
      "\n",
      "Activity Sequence: ER Registration -> ER Triage -> Leucocytes -> CRP -> LacticAcid -> Leucocytes\n",
      "Feature Values: [1, 1, 1, 1, 1, 0, 0, 1, 60, 1]\n",
      "Biomarker Values: {'Leucocytes': 'Elevated', 'CRP': 'Severe', 'LacticAcid': 'Normal'}\n",
      "Predicted Next Activity: ('Leucocytes', np.float64(32473.04))\n",
      "\n",
      "Activity Sequence: ER Registration -> ER Triage -> CRP -> LacticAcid -> CRP\n",
      "Feature Values: [0, 1, 1, 1, 1, 0, 0, 1, 40, 0]\n",
      "Biomarker Values: {'CRP': 'Severe', 'LacticAcid': 'Critical'}\n",
      "Predicted Next Activity: ('Leucocytes', np.float64(32473.04))\n",
      "\n",
      "Activity Sequence: ER Registration -> ER Triage -> LacticAcid -> CRP -> LacticAcid\n",
      "Feature Values: [1, 0, 1, 1, 1, 0, 0, 0, 55, 1]\n",
      "Biomarker Values: {'LacticAcid': 'Critical', 'CRP': 'Severe'}\n",
      "Predicted Next Activity: ('Leucocytes', np.float64(32473.04))\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Code\\GitHub\\Process_Analytics\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run predictions for each test case using predict_next_activity_and_time_new\n",
    "predictions = []\n",
    "\n",
    "for test_case in test_cases:\n",
    "    activity_sequence = test_case[\"activity_sequence\"]\n",
    "    feature_values = test_case[\"feature_values\"]\n",
    "    biomarker_values = test_case[\"biomarker_values\"]\n",
    "    predicted_next_activity = predict_next_activity_and_time_new(activity_sequence, feature_values, biomarker_values)\n",
    "\n",
    "    predictions.append({\n",
    "        \"activity_sequence\": activity_sequence,\n",
    "        \"feature_values\": feature_values,\n",
    "        \"biomarker_values\": biomarker_values,\n",
    "        \"predicted_next_activity\": predicted_next_activity\n",
    "    })\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# Display the predictions\n",
    "\n",
    "for prediction in predictions:\n",
    "    print(f\"Activity Sequence: {prediction['activity_sequence']}\")\n",
    "    print(f\"Feature Values: {prediction['feature_values']}\")\n",
    "    print(f\"Biomarker Values: {prediction['biomarker_values']}\")\n",
    "    print(f\"Predicted Next Activity: {prediction['predicted_next_activity']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model in Keras format (recommended)\n",
    "save_model(model, \"sepsis_lstm_model.keras\")\n",
    "\n",
    "# Save tokenizer\n",
    "with open(\"sepsis_tokenizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "# Save label encoder\n",
    "with open(\"sepsis_label_encoder.pkl\", \"wb\") as file:\n",
    "    pickle.dump(label_encoder, file)\n",
    "\n",
    "# Save feature scaler\n",
    "with open(\"sepsis_scaler.pkl\", \"wb\") as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "# Save time prediction model\n",
    "with open(\"sepsis_time_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(time_model, file)\n",
    "\n",
    "df_time_avg.to_csv(\"Sepsis_Avg_Activity_Duration.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
